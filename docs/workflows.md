# å·¥ä½œæµç¨‹ç¯„ä¾‹æŒ‡å—

**æ–‡æª”ç‰ˆæœ¬**: v2.0  
**é©ç”¨é ˜åŸŸ**: NGSåˆ†æã€è›‹ç™½è³ªçµæ§‹é æ¸¬ã€æ©Ÿå™¨å­¸ç¿’ã€è³‡æ–™ç§‘å­¸  
**ç›®æ¨™è®€è€…**: ç ”ç©¶è€…ã€åˆ†æå¸«ã€å­¸ç¿’è€…

## ğŸ§¬ NGS è³‡æ–™åˆ†æå·¥ä½œæµç¨‹

### åŸºæœ¬ NGS åˆ†ææµç¨‹

#### å®Œæ•´åŸºå› çµ„é‡æ¸¬åºåˆ†æ
```bash
#!/bin/bash
# wgs_analysis.sh - å…¨åŸºå› çµ„é‡æ¸¬åºåˆ†ææµç¨‹
# ç”¨æ³•: bash wgs_analysis.sh <sample_name> <fastq_dir> <reference_genome>

set -e  # é‡åˆ°éŒ¯èª¤å³åœæ­¢

SAMPLE_NAME="$1"
FASTQ_DIR="$2"
REFERENCE="$3"
OUTPUT_DIR="results/${SAMPLE_NAME}"

# æª¢æŸ¥åƒæ•¸
if [ $# -ne 3 ]; then
    echo "ç”¨æ³•: $0 <sample_name> <fastq_dir> <reference_genome>"
    echo "ç¯„ä¾‹: $0 sample01 data/fastq data/reference/hg38.fa"
    exit 1
fi

echo "ğŸ§¬ é–‹å§‹å…¨åŸºå› çµ„é‡æ¸¬åºåˆ†æ: $SAMPLE_NAME"
echo "================================================"

# å»ºç«‹è¼¸å‡ºç›®éŒ„
mkdir -p "$OUTPUT_DIR"/{qc,alignment,variants,reports}

# ============================================================================
# æ­¥é©Ÿ 1: åŸå§‹è³‡æ–™å“è³ªæ§åˆ¶
# ============================================================================
echo "ğŸ“Š æ­¥é©Ÿ 1/6: åŸå§‹è³‡æ–™å“è³ªæ§åˆ¶"
echo "è¼¸å…¥ç›®éŒ„: $FASTQ_DIR"

# FastQC å“è³ªæª¢æŸ¥
fastqc "$FASTQ_DIR"/${SAMPLE_NAME}*.fastq.gz -o "$OUTPUT_DIR/qc/" -t 4

# MultiQC æ•´åˆå ±å‘Š (å¦‚æœå¯ç”¨)
if command -v multiqc &> /dev/null; then
    multiqc "$OUTPUT_DIR/qc/" -o "$OUTPUT_DIR/reports/" -n "${SAMPLE_NAME}_fastqc_report"
fi

echo "âœ… å“è³ªæ§åˆ¶å®Œæˆ"

# ============================================================================  
# æ­¥é©Ÿ 2: è®€æ®µæ¯”å°
# ============================================================================
echo "ğŸ¯ æ­¥é©Ÿ 2/6: åºåˆ—æ¯”å°"

# æª¢æŸ¥åƒè€ƒåŸºå› çµ„ç´¢å¼•
if [ ! -f "${REFERENCE}.bwt" ]; then
    echo "å»ºç«‹ BWA ç´¢å¼•..."
    bwa index "$REFERENCE"
fi

# BWA æ¯”å° (å‡è¨­ç‚ºé…å°æœ«ç«¯æ¸¬åº)
READ1="$FASTQ_DIR/${SAMPLE_NAME}_R1.fastq.gz"
READ2="$FASTQ_DIR/${SAMPLE_NAME}_R2.fastq.gz"

if [ -f "$READ1" ] && [ -f "$READ2" ]; then
    echo "é…å°æœ«ç«¯æ¯”å°: $READ1, $READ2"
    bwa mem -t 8 -M "$REFERENCE" "$READ1" "$READ2" | \
    samtools view -Sb - | \
    samtools sort -@ 4 -o "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam"
else
    echo "âŒ æ‰¾ä¸åˆ°é…å°çš„ FASTQ æª”æ¡ˆ"
    exit 1
fi

# å»ºç«‹ BAM ç´¢å¼•
samtools index "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam"

echo "âœ… åºåˆ—æ¯”å°å®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 3: æ¯”å°å¾Œè™•ç†
# ============================================================================
echo "ğŸ”§ æ­¥é©Ÿ 3/6: æ¯”å°å¾Œè™•ç†"

# æ¨™è¨˜é‡è¤‡åºåˆ—
samtools markdup \
    "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam" \
    "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.markdup.bam"

samtools index "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.markdup.bam"

# æ¯”å°çµ±è¨ˆ
samtools flagstat "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.markdup.bam" > \
    "$OUTPUT_DIR/reports/${SAMPLE_NAME}.alignment_stats.txt"

samtools coverage "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.markdup.bam" > \
    "$OUTPUT_DIR/reports/${SAMPLE_NAME}.coverage_stats.txt"

echo "âœ… æ¯”å°å¾Œè™•ç†å®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 4: è®Šç•°æª¢æ¸¬ (ä½¿ç”¨ GATK)
# ============================================================================
echo "ğŸ”¬ æ­¥é©Ÿ 4/6: è®Šç•°æª¢æ¸¬"

# ç¢ºä¿ GATK å®¹å™¨å¯ç”¨
bash .devcontainer/scripts/manage/tool-manager.sh manage start gatk

# GATK HaplotypeCaller è®Šç•°å‘¼å«
bash .devcontainer/scripts/manage/tool-manager.sh gatk HaplotypeCaller \
    -I "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.markdup.bam" \
    -O "$OUTPUT_DIR/variants/${SAMPLE_NAME}.raw.vcf.gz" \
    -R "$REFERENCE" \
    --emit-ref-confidence GVCF

echo "âœ… è®Šç•°æª¢æ¸¬å®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 5: è®Šç•°éæ¿¾
# ============================================================================
echo "ğŸ” æ­¥é©Ÿ 5/6: è®Šç•°éæ¿¾"

# åŸºæœ¬å“è³ªéæ¿¾
bash .devcontainer/scripts/manage/tool-manager.sh gatk VariantFiltration \
    -V "$OUTPUT_DIR/variants/${SAMPLE_NAME}.raw.vcf.gz" \
    -O "$OUTPUT_DIR/variants/${SAMPLE_NAME}.filtered.vcf.gz" \
    --filter-expression "QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0" \
    --filter-name "basic_filter"

# é€²ä¸€æ­¥éæ¿¾ (ç§»é™¤ä½å“è³ªè®Šç•°)
bcftools filter -i 'FILTER="PASS"' \
    "$OUTPUT_DIR/variants/${SAMPLE_NAME}.filtered.vcf.gz" \
    -o "$OUTPUT_DIR/variants/${SAMPLE_NAME}.final.vcf.gz"

echo "âœ… è®Šç•°éæ¿¾å®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 6: çµæœçµ±è¨ˆèˆ‡å ±å‘Š
# ============================================================================
echo "ğŸ“ˆ æ­¥é©Ÿ 6/6: ç”Ÿæˆåˆ†æå ±å‘Š"

# VCF çµ±è¨ˆ
bcftools stats "$OUTPUT_DIR/variants/${SAMPLE_NAME}.final.vcf.gz" > \
    "$OUTPUT_DIR/reports/${SAMPLE_NAME}.variant_stats.txt"

# è®Šç•°é¡å‹çµ±è¨ˆ
bcftools query -f '%TYPE\n' "$OUTPUT_DIR/variants/${SAMPLE_NAME}.final.vcf.gz" | \
    sort | uniq -c > "$OUTPUT_DIR/reports/${SAMPLE_NAME}.variant_types.txt"

# ç”Ÿæˆæœ€çµ‚å ±å‘Š
cat > "$OUTPUT_DIR/reports/${SAMPLE_NAME}_final_report.txt" << EOF
========================================
NGS åˆ†æå ±å‘Š: $SAMPLE_NAME
========================================
åˆ†ææ—¥æœŸ: $(date)
åƒè€ƒåŸºå› çµ„: $REFERENCE

æª”æ¡ˆä½ç½®:
- æ¯”å°æª”æ¡ˆ: $OUTPUT_DIR/alignment/${SAMPLE_NAME}.markdup.bam
- è®Šç•°æª”æ¡ˆ: $OUTPUT_DIR/variants/${SAMPLE_NAME}.final.vcf.gz
- å“è³ªå ±å‘Š: $OUTPUT_DIR/qc/
- çµ±è¨ˆå ±å‘Š: $OUTPUT_DIR/reports/

ä¸»è¦çµ±è¨ˆ:
$(cat "$OUTPUT_DIR/reports/${SAMPLE_NAME}.alignment_stats.txt")

è®Šç•°çµ±è¨ˆ:
$(head -20 "$OUTPUT_DIR/reports/${SAMPLE_NAME}.variant_stats.txt")
========================================
EOF

echo "âœ… NGS åˆ†ææµç¨‹å®Œæˆ!"
echo "ğŸ“‚ çµæœä½ç½®: $OUTPUT_DIR"
echo "ğŸ“‹ æª¢è¦–å ±å‘Š: $OUTPUT_DIR/reports/${SAMPLE_NAME}_final_report.txt"
```

### RNA-seq è¡¨é”åˆ†ææµç¨‹
```bash
#!/bin/bash
# rnaseq_analysis.sh - RNA-seq è¡¨é”åˆ†ææµç¨‹

SAMPLE_NAME="$1"
FASTQ_DIR="$2"
REFERENCE_GENOME="$3"
ANNOTATION_GTF="$4"
OUTPUT_DIR="results/rnaseq/${SAMPLE_NAME}"

echo "ğŸ§¬ é–‹å§‹ RNA-seq è¡¨é”åˆ†æ: $SAMPLE_NAME"

mkdir -p "$OUTPUT_DIR"/{qc,alignment,counts,reports}

# 1. å“è³ªæ§åˆ¶
echo "ğŸ“Š æ­¥é©Ÿ 1: å“è³ªæ§åˆ¶"
fastqc "$FASTQ_DIR"/${SAMPLE_NAME}*.fastq.gz -o "$OUTPUT_DIR/qc/"

# 2. åºåˆ—æ¯”å° (ä½¿ç”¨ STAR æˆ– HISAT2)
echo "ğŸ¯ æ­¥é©Ÿ 2: RNA-seq æ¯”å°"
if command -v hisat2 &> /dev/null; then
    # ä½¿ç”¨ HISAT2
    hisat2 -x "$REFERENCE_GENOME" \
           -1 "$FASTQ_DIR/${SAMPLE_NAME}_R1.fastq.gz" \
           -2 "$FASTQ_DIR/${SAMPLE_NAME}_R2.fastq.gz" \
           -S "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sam" \
           --threads 8
    
    # è½‰æ›ç‚º BAM ä¸¦æ’åº
    samtools view -bS "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sam" | \
    samtools sort -o "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam"
    
    samtools index "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam"
    rm "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sam"  # ç¯€çœç©ºé–“
fi

# 3. åŸºå› è¨ˆæ•¸
echo "ğŸ”¢ æ­¥é©Ÿ 3: åŸºå› è¡¨é”è¨ˆæ•¸"
if command -v featureCounts &> /dev/null; then
    featureCounts -a "$ANNOTATION_GTF" \
                  -o "$OUTPUT_DIR/counts/${SAMPLE_NAME}.counts.txt" \
                  -T 8 \
                  -p \
                  "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam"
fi

# 4. è¡¨é”çµ±è¨ˆ
echo "ğŸ“Š æ­¥é©Ÿ 4: è¡¨é”çµ±è¨ˆ"
samtools flagstat "$OUTPUT_DIR/alignment/${SAMPLE_NAME}.sorted.bam" > \
    "$OUTPUT_DIR/reports/${SAMPLE_NAME}.alignment_stats.txt"

echo "âœ… RNA-seq åˆ†æå®Œæˆ!"
```

## ğŸ§ª è›‹ç™½è³ªçµæ§‹é æ¸¬å·¥ä½œæµç¨‹

### å–®ä¸€è›‹ç™½è³ªçµæ§‹é æ¸¬
```bash
#!/bin/bash
# protein_structure_prediction.sh - è›‹ç™½è³ªçµæ§‹é æ¸¬æµç¨‹

PROTEIN_FASTA="$1"
PROTEIN_NAME="$2"
OUTPUT_DIR="results/structure/${PROTEIN_NAME}"

if [ $# -ne 2 ]; then
    echo "ç”¨æ³•: $0 <protein.fasta> <protein_name>"
    exit 1
fi

echo "ğŸ§¬ é–‹å§‹è›‹ç™½è³ªçµæ§‹é æ¸¬: $PROTEIN_NAME"
echo "============================================"

mkdir -p "$OUTPUT_DIR"/{models,analysis,reports}

# ============================================================================
# æ­¥é©Ÿ 1: åºåˆ—é©—è­‰
# ============================================================================
echo "ğŸ” æ­¥é©Ÿ 1/5: åºåˆ—é©—è­‰"

# æª¢æŸ¥ FASTA æ ¼å¼
if ! grep -q "^>" "$PROTEIN_FASTA"; then
    echo "âŒ ç„¡æ•ˆçš„ FASTA æ ¼å¼"
    exit 1
fi

# åºåˆ—çµ±è¨ˆ
SEQUENCE_LENGTH=$(grep -v "^>" "$PROTEIN_FASTA" | tr -d '\n' | wc -c)
echo "åºåˆ—é•·åº¦: $SEQUENCE_LENGTH å€‹æ°¨åŸºé…¸"

if [ "$SEQUENCE_LENGTH" -gt 2000 ]; then
    echo "âš ï¸  è­¦å‘Š: åºåˆ—è¼ƒé•· (>2000 AA)ï¼Œé æ¸¬æ™‚é–“å¯èƒ½å¾ˆé•·"
fi

echo "âœ… åºåˆ—é©—è­‰å®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 2: GPU ç’°å¢ƒæª¢æŸ¥
# ============================================================================
echo "ğŸ® æ­¥é©Ÿ 2/5: GPU ç’°å¢ƒæª¢æŸ¥"

if nvidia-smi &>/dev/null; then
    echo "âœ… NVIDIA GPU å¯ç”¨"
    GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    echo "GPU è¨˜æ†¶é«”: ${GPU_MEMORY} MB"
    
    if [ "$GPU_MEMORY" -lt 8000 ]; then
        echo "âš ï¸  è­¦å‘Š: GPU è¨˜æ†¶é«”å¯èƒ½ä¸è¶³ï¼Œå»ºè­°ä½¿ç”¨ CPU æ¨¡å¼"
        USE_GPU=false
    else
        USE_GPU=true
    fi
else
    echo "â„¹ï¸  æœªæª¢æ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU æ¨¡å¼"
    USE_GPU=false
fi

# ============================================================================
# æ­¥é©Ÿ 3: ColabFold çµæ§‹é æ¸¬
# ============================================================================
echo "ğŸ”¬ æ­¥é©Ÿ 3/5: ColabFold çµæ§‹é æ¸¬"

# ç¢ºä¿ ColabFold å®¹å™¨å¯ç”¨
bash .devcontainer/scripts/manage/tool-manager.sh check colabfold

# è¨­å®šé æ¸¬åƒæ•¸
if [ "$USE_GPU" = true ]; then
    COLABFOLD_ARGS="--use-gpu-relax --num-models 5 --num-recycles 3"
else
    COLABFOLD_ARGS="--num-models 3 --num-recycles 1"
fi

# åŸ·è¡Œçµæ§‹é æ¸¬
bash .devcontainer/scripts/manage/tool-manager.sh colabfold \
    "$PROTEIN_FASTA" \
    "$OUTPUT_DIR/models/" \
    $COLABFOLD_ARGS

echo "âœ… çµæ§‹é æ¸¬å®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 4: çµæœåˆ†æ
# ============================================================================
echo "ğŸ“Š æ­¥é©Ÿ 4/5: çµæœåˆ†æ"

# æ‰¾åˆ°æœ€ä½³æ¨¡å‹ (é€šå¸¸æ˜¯ rank_001)
BEST_MODEL=$(find "$OUTPUT_DIR/models/" -name "*rank_001*.pdb" | head -1)

if [ -n "$BEST_MODEL" ]; then
    echo "æœ€ä½³æ¨¡å‹: $BEST_MODEL"
    
    # è¤‡è£½æœ€ä½³æ¨¡å‹åˆ°åˆ†æç›®éŒ„
    cp "$BEST_MODEL" "$OUTPUT_DIR/analysis/${PROTEIN_NAME}_best_model.pdb"
    
    # æå–ä¿¡å¿ƒåˆ†æ•¸ (å¦‚æœæœ‰ JSON æª”æ¡ˆ)
    JSON_FILE="${BEST_MODEL%.pdb}.json"
    if [ -f "$JSON_FILE" ]; then
        python3 << EOF
import json
import numpy as np

with open('$JSON_FILE', 'r') as f:
    data = json.load(f)

if 'confidenceScore' in data:
    scores = data['confidenceScore']
    avg_score = np.mean(scores)
    print(f"å¹³å‡ä¿¡å¿ƒåˆ†æ•¸: {avg_score:.2f}")
    
    high_conf = sum(1 for s in scores if s > 90)
    total = len(scores)
    print(f"é«˜ä¿¡å¿ƒå€åŸŸ: {high_conf}/{total} ({high_conf/total*100:.1f}%)")
EOF
    fi
else
    echo "âŒ æœªæ‰¾åˆ°é æ¸¬æ¨¡å‹"
fi

# çµ±è¨ˆæ‰€æœ‰æ¨¡å‹
MODEL_COUNT=$(find "$OUTPUT_DIR/models/" -name "*.pdb" | wc -l)
echo "ç”Ÿæˆæ¨¡å‹æ•¸é‡: $MODEL_COUNT"

echo "âœ… çµæœåˆ†æå®Œæˆ"

# ============================================================================
# æ­¥é©Ÿ 5: ç”Ÿæˆå ±å‘Š
# ============================================================================
echo "ğŸ“‹ æ­¥é©Ÿ 5/5: ç”Ÿæˆåˆ†æå ±å‘Š"

cat > "$OUTPUT_DIR/reports/${PROTEIN_NAME}_structure_report.txt" << EOF
========================================
è›‹ç™½è³ªçµæ§‹é æ¸¬å ±å‘Š
========================================
è›‹ç™½è³ªåç¨±: $PROTEIN_NAME
è¼¸å…¥æª”æ¡ˆ: $PROTEIN_FASTA
åºåˆ—é•·åº¦: $SEQUENCE_LENGTH å€‹æ°¨åŸºé…¸
é æ¸¬æ—¥æœŸ: $(date)
ä½¿ç”¨ GPU: $USE_GPU

é æ¸¬çµæœ:
- æ¨¡å‹æ•¸é‡: $MODEL_COUNT
- æœ€ä½³æ¨¡å‹: $(basename "$BEST_MODEL" 2>/dev/null || echo "ç„¡")
- çµæœç›®éŒ„: $OUTPUT_DIR/models/

å»ºè­°å¾ŒçºŒåˆ†æ:
1. ä½¿ç”¨ PyMOL æˆ– ChimeraX è¦–è¦ºåŒ–çµæ§‹
2. é€²è¡Œçµæ§‹å“è³ªè©•ä¼° (Ramachandran plot)
3. åŠŸèƒ½åŸŸåˆ†æå’Œæ´»æ€§ä½é»é æ¸¬
4. èˆ‡å·²çŸ¥çµæ§‹é€²è¡Œæ¯”è¼ƒ (å¦‚æœ‰éœ€è¦)

æª”æ¡ˆèªªæ˜:
- *.pdb: è›‹ç™½è³ªçµæ§‹æª”æ¡ˆ
- *.json: é æ¸¬ä¿¡å¿ƒåˆ†æ•¸å’Œå…ƒè³‡æ–™
- *_coverage.png: MSA è¦†è“‹ç‡åœ–è¡¨ (å¦‚æœ‰)
- *_PAE.png: é æ¸¬æ ¡æº–èª¤å·®åœ–è¡¨ (å¦‚æœ‰)
========================================
EOF

echo "âœ… è›‹ç™½è³ªçµæ§‹é æ¸¬æµç¨‹å®Œæˆ!"
echo "ğŸ“‚ çµæœä½ç½®: $OUTPUT_DIR"
echo "ğŸ“‹ æŸ¥çœ‹å ±å‘Š: $OUTPUT_DIR/reports/${PROTEIN_NAME}_structure_report.txt"

# å¿«é€Ÿæª¢è¦–æœ€ä½³æ¨¡å‹ (å¦‚æœå®‰è£äº† PyMOL)
if command -v pymol &> /dev/null && [ -n "$BEST_MODEL" ]; then
    echo "ğŸ”¬ æ˜¯å¦è¦ç”¨ PyMOL é–‹å•Ÿçµæ§‹? (y/N)"
    read -r response
    if [[ "$response" =~ ^[Yy]$ ]]; then
        pymol "$BEST_MODEL" &
    fi
fi
```

### æ‰¹æ¬¡è›‹ç™½è³ªçµæ§‹é æ¸¬
```bash
#!/bin/bash
# batch_structure_prediction.sh - æ‰¹æ¬¡è›‹ç™½è³ªçµæ§‹é æ¸¬

FASTA_DIR="$1"
OUTPUT_BASE="results/batch_structure"

if [ ! -d "$FASTA_DIR" ]; then
    echo "ç”¨æ³•: $0 <fasta_directory>"
    exit 1
fi

echo "ğŸ§¬ é–‹å§‹æ‰¹æ¬¡è›‹ç™½è³ªçµæ§‹é æ¸¬"
echo "è¼¸å…¥ç›®éŒ„: $FASTA_DIR"

mkdir -p "$OUTPUT_BASE"

# æ‰¾åˆ°æ‰€æœ‰ FASTA æª”æ¡ˆ
FASTA_FILES=($(find "$FASTA_DIR" -name "*.fasta" -o -name "*.fa"))

if [ ${#FASTA_FILES[@]} -eq 0 ]; then
    echo "âŒ åœ¨ $FASTA_DIR ä¸­æœªæ‰¾åˆ° FASTA æª”æ¡ˆ"
    exit 1
fi

echo "æ‰¾åˆ° ${#FASTA_FILES[@]} å€‹ FASTA æª”æ¡ˆ"

# æ‰¹æ¬¡è™•ç†
for fasta_file in "${FASTA_FILES[@]}"; do
    protein_name=$(basename "$fasta_file" | sed 's/\.\(fasta\|fa\)$//')
    echo "è™•ç†: $protein_name"
    
    # å‘¼å«å–®ä¸€è›‹ç™½è³ªé æ¸¬è…³æœ¬
    bash protein_structure_prediction.sh "$fasta_file" "$protein_name"
    
    echo "å®Œæˆ: $protein_name"
    echo "----------------------------------------"
done

echo "âœ… æ‰¹æ¬¡çµæ§‹é æ¸¬å®Œæˆ!"
```

## ğŸ¤– æ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹

### åœ–åƒåˆ†é¡æ¨¡å‹è¨“ç·´
```python
#!/usr/bin/env python3
# image_classification_pipeline.py - åœ–åƒåˆ†é¡å®Œæ•´æµç¨‹

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models
import seaborn as sns

def setup_environment():
    """è¨­ç½®ç’°å¢ƒå’Œ GPU"""
    print("ğŸ”§ è¨­ç½®åŸ·è¡Œç’°å¢ƒ")
    
    # æª¢æŸ¥ GPU å¯ç”¨æ€§
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        print(f"âœ… ç™¼ç¾ {len(gpus)} å€‹ GPU")
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    else:
        print("â„¹ï¸  ä½¿ç”¨ CPU æ¨¡å¼")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    tf.random.set_seed(42)
    np.random.seed(42)

def load_and_preprocess_data(data_dir, img_size=(224, 224), batch_size=32):
    """è¼‰å…¥å’Œé è™•ç†è³‡æ–™"""
    print("ğŸ“Š è¼‰å…¥å’Œé è™•ç†è³‡æ–™")
    
    # è³‡æ–™å¢å¼·
    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255,
        rotation_range=20,
        width_shift_range=0.2,
        height_shift_range=0.2,
        horizontal_flip=True,
        validation_split=0.2
    )
    
    # è¨“ç·´è³‡æ–™
    train_data = train_datagen.flow_from_directory(
        data_dir,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='training'
    )
    
    # é©—è­‰è³‡æ–™
    val_data = train_datagen.flow_from_directory(
        data_dir,
        target_size=img_size,
        batch_size=batch_size,
        class_mode='categorical',
        subset='validation'
    )
    
    return train_data, val_data

def create_model(input_shape, num_classes):
    """å»ºç«‹ CNN æ¨¡å‹"""
    print("ğŸ—ï¸  å»ºç«‹ CNN æ¨¡å‹")
    
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(128, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(512, activation='relu'),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model

def train_model(model, train_data, val_data, epochs=20):
    """è¨“ç·´æ¨¡å‹"""
    print("ğŸš€ é–‹å§‹æ¨¡å‹è¨“ç·´")
    
    # å›èª¿å‡½æ•¸
    callbacks = [
        tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.2,
            patience=3,
            min_lr=0.0001
        ),
        tf.keras.callbacks.ModelCheckpoint(
            'results/models/best_model.h5',
            monitor='val_accuracy',
            save_best_only=True
        )
    ]
    
    # è¨“ç·´
    history = model.fit(
        train_data,
        validation_data=val_data,
        epochs=epochs,
        callbacks=callbacks,
        verbose=1
    )
    
    return history

def evaluate_and_visualize(model, val_data, class_names, output_dir):
    """è©•ä¼°æ¨¡å‹ä¸¦ç”Ÿæˆè¦–è¦ºåŒ–"""
    print("ğŸ“Š è©•ä¼°æ¨¡å‹æ•ˆèƒ½")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # é æ¸¬
    predictions = model.predict(val_data)
    y_pred = np.argmax(predictions, axis=1)
    y_true = val_data.classes
    
    # åˆ†é¡å ±å‘Š
    report = classification_report(y_true, y_pred, target_names=class_names)
    print("\nåˆ†é¡å ±å‘Š:")
    print(report)
    
    # å„²å­˜å ±å‘Š
    with open(f"{output_dir}/classification_report.txt", "w") as f:
        f.write(report)
    
    # æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title('æ··æ·†çŸ©é™£')
    plt.ylabel('çœŸå¯¦æ¨™ç±¤')
    plt.xlabel('é æ¸¬æ¨™ç±¤')
    plt.savefig(f"{output_dir}/confusion_matrix.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    return report

def plot_training_history(history, output_dir):
    """ç¹ªè£½è¨“ç·´æ­·å²"""
    print("ğŸ“ˆ ç”Ÿæˆè¨“ç·´æ­·å²åœ–è¡¨")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    
    # æº–ç¢ºç‡
    ax1.plot(history.history['accuracy'], label='è¨“ç·´æº–ç¢ºç‡')
    ax1.plot(history.history['val_accuracy'], label='é©—è­‰æº–ç¢ºç‡')
    ax1.set_title('æ¨¡å‹æº–ç¢ºç‡')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('æº–ç¢ºç‡')
    ax1.legend()
    
    # æå¤±
    ax2.plot(history.history['loss'], label='è¨“ç·´æå¤±')
    ax2.plot(history.history['val_loss'], label='é©—è­‰æå¤±')
    ax2.set_title('æ¨¡å‹æå¤±')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('æå¤±')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/training_history.png", dpi=300, bbox_inches='tight')
    plt.close()

def main():
    """ä¸»è¦åŸ·è¡Œæµç¨‹"""
    print("ğŸ¤– åœ–åƒåˆ†é¡æ©Ÿå™¨å­¸ç¿’æµç¨‹")
    print("=" * 40)
    
    # è¨­å®šåƒæ•¸
    DATA_DIR = "data/images"  # åœ–åƒè³‡æ–™ç›®éŒ„
    OUTPUT_DIR = "results/image_classification"
    IMG_SIZE = (224, 224)
    BATCH_SIZE = 32
    EPOCHS = 20
    
    # æª¢æŸ¥è³‡æ–™ç›®éŒ„
    if not os.path.exists(DATA_DIR):
        print(f"âŒ è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {DATA_DIR}")
        print("è«‹ç¢ºä¿è³‡æ–™ç›®éŒ„çµæ§‹å¦‚ä¸‹:")
        print("data/images/")
        print("â”œâ”€â”€ class1/")
        print("â”œâ”€â”€ class2/")
        print("â””â”€â”€ class3/")
        return
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(f"{OUTPUT_DIR}/models", exist_ok=True)
    
    # åŸ·è¡Œæµç¨‹
    setup_environment()
    
    train_data, val_data = load_and_preprocess_data(
        DATA_DIR, IMG_SIZE, BATCH_SIZE
    )
    
    # å–å¾—é¡åˆ¥è³‡è¨Š
    class_names = list(train_data.class_indices.keys())
    num_classes = len(class_names)
    print(f"é¡åˆ¥æ•¸é‡: {num_classes}")
    print(f"é¡åˆ¥åç¨±: {class_names}")
    
    # å»ºç«‹å’Œè¨“ç·´æ¨¡å‹
    model = create_model((*IMG_SIZE, 3), num_classes)
    print(model.summary())
    
    history = train_model(model, train_data, val_data, EPOCHS)
    
    # è©•ä¼°å’Œè¦–è¦ºåŒ–
    evaluate_and_visualize(model, val_data, class_names, OUTPUT_DIR)
    plot_training_history(history, OUTPUT_DIR)
    
    # å„²å­˜æœ€çµ‚æ¨¡å‹
    model.save(f"{OUTPUT_DIR}/models/final_model.h5")
    
    print("âœ… åœ–åƒåˆ†é¡æµç¨‹å®Œæˆ!")
    print(f"ğŸ“‚ çµæœä½ç½®: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
```

### è‡ªç„¶èªè¨€è™•ç†æµç¨‹
```python
#!/usr/bin/env python3
# nlp_sentiment_analysis.py - æƒ…æ„Ÿåˆ†ææµç¨‹

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
import torch
from torch.utils.data import Dataset
import os

class SentimentDataset(Dataset):
    """è‡ªè¨‚è³‡æ–™é›†é¡åˆ¥"""
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

def load_and_preprocess_data(data_file):
    """è¼‰å…¥å’Œé è™•ç†æ–‡æœ¬è³‡æ–™"""
    print("ğŸ“Š è¼‰å…¥å’Œé è™•ç†æ–‡æœ¬è³‡æ–™")
    
    # è¼‰å…¥ CSV è³‡æ–™ (å‡è¨­æœ‰ 'text' å’Œ 'sentiment' æ¬„ä½)
    df = pd.read_csv(data_file)
    
    # è³‡æ–™æ¸…ç†
    df = df.dropna(subset=['text', 'sentiment'])
    df['text'] = df['text'].astype(str)
    
    # æ¨™ç±¤ç·¨ç¢¼
    label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
    df['label'] = df['sentiment'].map(label_mapping)
    
    # ç§»é™¤æœªçŸ¥æ¨™ç±¤
    df = df.dropna(subset=['label'])
    df['label'] = df['label'].astype(int)
    
    print(f"è³‡æ–™é‡: {len(df)} æ¢")
    print("æ¨™ç±¤åˆ†å¸ƒ:")
    print(df['sentiment'].value_counts())
    
    return df

def setup_model_and_tokenizer(model_name="bert-base-chinese"):
    """è¨­ç½®æ¨¡å‹å’Œåˆ†è©å™¨"""
    print(f"ğŸ¤– è¼‰å…¥é è¨“ç·´æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name, 
        num_labels=3
    )
    
    return model, tokenizer

def train_sentiment_model(model, train_dataset, val_dataset, output_dir):
    """è¨“ç·´æƒ…æ„Ÿåˆ†ææ¨¡å‹"""
    print("ğŸš€ é–‹å§‹æ¨¡å‹å¾®èª¿")
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=f'{output_dir}/logs',
        logging_steps=100,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )
    
    trainer.train()
    return trainer

def evaluate_model(trainer, test_dataset, output_dir):
    """è©•ä¼°æ¨¡å‹"""
    print("ğŸ“Š è©•ä¼°æ¨¡å‹æ•ˆèƒ½")
    
    # é æ¸¬
    predictions = trainer.predict(test_dataset)
    y_pred = np.argmax(predictions.predictions, axis=1)
    y_true = predictions.label_ids
    
    # åˆ†é¡å ±å‘Š
    label_names = ['Negative', 'Neutral', 'Positive']
    report = classification_report(y_true, y_pred, target_names=label_names)
    print("\nåˆ†é¡å ±å‘Š:")
    print(report)
    
    # å„²å­˜å ±å‘Š
    with open(f"{output_dir}/evaluation_report.txt", "w") as f:
        f.write(report)
    
    # æ··æ·†çŸ©é™£
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_names, yticklabels=label_names)
    plt.title('æƒ…æ„Ÿåˆ†ææ··æ·†çŸ©é™£')
    plt.ylabel('çœŸå¯¦æ¨™ç±¤')
    plt.xlabel('é æ¸¬æ¨™ç±¤')
    plt.savefig(f"{output_dir}/confusion_matrix.png", dpi=300, bbox_inches='tight')
    plt.close()

def main():
    """ä¸»è¦åŸ·è¡Œæµç¨‹"""
    print("ğŸ¤– è‡ªç„¶èªè¨€è™•ç† - æƒ…æ„Ÿåˆ†ææµç¨‹")
    print("=" * 40)
    
    # è¨­å®šåƒæ•¸
    DATA_FILE = "data/sentiment_data.csv"
    OUTPUT_DIR = "results/sentiment_analysis"
    MODEL_NAME = "bert-base-chinese"  # æˆ–ä½¿ç”¨ "bert-base-uncased" å°æ–¼è‹±æ–‡
    
    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
    if not os.path.exists(DATA_FILE):
        print(f"âŒ è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {DATA_FILE}")
        print("è«‹æº–å‚™åŒ…å« 'text' å’Œ 'sentiment' æ¬„ä½çš„ CSV æª”æ¡ˆ")
        return
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åŸ·è¡Œæµç¨‹
    df = load_and_preprocess_data(DATA_FILE)
    
    # åˆ†å‰²è³‡æ–™
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])
    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['label'])
    
    print(f"è¨“ç·´é›†: {len(train_df)} æ¢")
    print(f"é©—è­‰é›†: {len(val_df)} æ¢")
    print(f"æ¸¬è©¦é›†: {len(test_df)} æ¢")
    
    # è¨­ç½®æ¨¡å‹
    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)
    
    # å»ºç«‹è³‡æ–™é›†
    train_dataset = SentimentDataset(
        train_df['text'].values, train_df['label'].values, tokenizer
    )
    val_dataset = SentimentDataset(
        val_df['text'].values, val_df['label'].values, tokenizer
    )
    test_dataset = SentimentDataset(
        test_df['text'].values, test_df['label'].values, tokenizer
    )
    
    # è¨“ç·´æ¨¡å‹
    trainer = train_sentiment_model(model, train_dataset, val_dataset, OUTPUT_DIR)
    
    # è©•ä¼°æ¨¡å‹
    evaluate_model(trainer, test_dataset, OUTPUT_DIR)
    
    # å„²å­˜æ¨¡å‹
    trainer.save_model(f"{OUTPUT_DIR}/final_model")
    tokenizer.save_pretrained(f"{OUTPUT_DIR}/final_model")
    
    print("âœ… æƒ…æ„Ÿåˆ†ææµç¨‹å®Œæˆ!")
    print(f"ğŸ“‚ çµæœä½ç½®: {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
```

## ğŸ“Š è³‡æ–™ç§‘å­¸åˆ†ææµç¨‹

### æ¢ç´¢æ€§è³‡æ–™åˆ†æ (EDA)
```python
#!/usr/bin/env python3
# exploratory_data_analysis.py - æ¢ç´¢æ€§è³‡æ–™åˆ†ææµç¨‹

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os

def load_and_inspect_data(data_file):
    """è¼‰å…¥å’Œåˆæ­¥æª¢è¦–è³‡æ–™"""
    print("ğŸ“Š è¼‰å…¥å’Œåˆæ­¥æª¢è¦–è³‡æ–™")
    
    # è¼‰å…¥è³‡æ–™
    df = pd.read_csv(data_file)
    
    print(f"è³‡æ–™å½¢ç‹€: {df.shape}")
    print(f"æ¬„ä½æ•¸é‡: {df.shape[1]}")
    print(f"è¨˜éŒ„æ•¸é‡: {df.shape[0]}")
    
    print("\nåŸºæœ¬è³‡è¨Š:")
    print(df.info())
    
    print("\nå‰5ç­†è¨˜éŒ„:")
    print(df.head())
    
    print("\næ•¸å€¼æ¬„ä½çµ±è¨ˆ:")
    print(df.describe())
    
    return df

def data_quality_analysis(df, output_dir):
    """è³‡æ–™å“è³ªåˆ†æ"""
    print("ğŸ” è³‡æ–™å“è³ªåˆ†æ")
    
    # ç¼ºå¤±å€¼åˆ†æ
    missing_data = df.isnull().sum()
    missing_percent = 100 * missing_data / len(df)
    
    missing_df = pd.DataFrame({
        'ç¼ºå¤±æ•¸é‡': missing_data,
        'ç¼ºå¤±ç™¾åˆ†æ¯”': missing_percent
    })
    missing_df = missing_df[missing_df['ç¼ºå¤±æ•¸é‡'] > 0].sort_values('ç¼ºå¤±æ•¸é‡', ascending=False)
    
    if len(missing_df) > 0:
        print("\nç¼ºå¤±å€¼çµ±è¨ˆ:")
        print(missing_df)
        
        # ç¼ºå¤±å€¼è¦–è¦ºåŒ–
        plt.figure(figsize=(10, 6))
        missing_df['ç¼ºå¤±ç™¾åˆ†æ¯”'].plot(kind='bar')
        plt.title('å„æ¬„ä½ç¼ºå¤±å€¼ç™¾åˆ†æ¯”')
        plt.ylabel('ç¼ºå¤±ç™¾åˆ†æ¯” (%)')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/missing_values.png", dpi=300, bbox_inches='tight')
        plt.close()
    else:
        print("âœ… ç„¡ç¼ºå¤±å€¼")
    
    # é‡è¤‡å€¼æª¢æŸ¥
    duplicates = df.duplicated().sum()
    print(f"\né‡è¤‡è¨˜éŒ„: {duplicates} ç­† ({100*duplicates/len(df):.2f}%)")
    
    # è³‡æ–™é¡å‹åˆ†æ
    print("\nè³‡æ–™é¡å‹åˆ†å¸ƒ:")
    print(df.dtypes.value_counts())
    
    return missing_df

def numerical_analysis(df, output_dir):
    """æ•¸å€¼è®Šæ•¸åˆ†æ"""
    print("ğŸ“ˆ æ•¸å€¼è®Šæ•¸åˆ†æ")
    
    # é¸æ“‡æ•¸å€¼æ¬„ä½
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) == 0:
        print("ç„¡æ•¸å€¼æ¬„ä½")
        return
    
    print(f"æ•¸å€¼æ¬„ä½: {numeric_cols}")
    
    # åˆ†å¸ƒåˆ†æ
    n_cols = min(3, len(numeric_cols))
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    if n_rows == 1:
        axes = [axes] if n_cols == 1 else axes
    else:
        axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            axes[i].hist(df[col].dropna(), bins=30, alpha=0.7)
            axes[i].set_title(f'{col} åˆ†å¸ƒ')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('é »ç‡')
    
    # éš±è—å¤šé¤˜çš„å­åœ–
    for i in range(len(numeric_cols), len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/numerical_distributions.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # ç›¸é—œæ€§åˆ†æ
    if len(numeric_cols) > 1:
        correlation_matrix = df[numeric_cols].corr()
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                    square=True, fmt='.2f')
        plt.title('æ•¸å€¼è®Šæ•¸ç›¸é—œæ€§çŸ©é™£')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/correlation_matrix.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # æ‰¾å‡ºé«˜ç›¸é—œæ€§é…å°
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = correlation_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:  # é«˜ç›¸é—œæ€§é–¾å€¼
                    high_corr_pairs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        corr_val
                    ))
        
        if high_corr_pairs:
            print("\né«˜ç›¸é—œæ€§è®Šæ•¸é…å° (|r| > 0.7):")
            for var1, var2, corr in high_corr_pairs:
                print(f"  {var1} - {var2}: {corr:.3f}")

def categorical_analysis(df, output_dir):
    """é¡åˆ¥è®Šæ•¸åˆ†æ"""
    print("ğŸ“Š é¡åˆ¥è®Šæ•¸åˆ†æ")
    
    # é¸æ“‡é¡åˆ¥æ¬„ä½
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if len(categorical_cols) == 0:
        print("ç„¡é¡åˆ¥æ¬„ä½")
        return
    
    print(f"é¡åˆ¥æ¬„ä½: {categorical_cols}")
    
    # åˆ†ææ¯å€‹é¡åˆ¥è®Šæ•¸
    for col in categorical_cols[:4]:  # é™åˆ¶æœ€å¤š4å€‹æ¬„ä½
        print(f"\n{col} åˆ†å¸ƒ:")
        value_counts = df[col].value_counts()
        print(value_counts.head(10))
        
        # é•·æ¢åœ–
        plt.figure(figsize=(10, 6))
        if len(value_counts) <= 20:  # é¡åˆ¥ä¸å¤ªå¤šæ™‚é¡¯ç¤ºæ‰€æœ‰é¡åˆ¥
            value_counts.plot(kind='bar')
            plt.xticks(rotation=45)
        else:  # é¡åˆ¥å¤ªå¤šæ™‚åªé¡¯ç¤ºå‰20å€‹
            value_counts.head(20).plot(kind='bar')
            plt.xticks(rotation=45)
            plt.title(f'{col} åˆ†å¸ƒ (å‰20å€‹é¡åˆ¥)')
        
        plt.title(f'{col} åˆ†å¸ƒ')
        plt.ylabel('é »ç‡')
        plt.tight_layout()
        plt.savefig(f"{output_dir}/categorical_{col.replace('/', '_')}.png", dpi=300, bbox_inches='tight')
        plt.close()

def outlier_analysis(df, output_dir):
    """ç•°å¸¸å€¼åˆ†æ"""
    print("ğŸ¯ ç•°å¸¸å€¼åˆ†æ")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) == 0:
        print("ç„¡æ•¸å€¼æ¬„ä½é€²è¡Œç•°å¸¸å€¼åˆ†æ")
        return
    
    # ç®±å‹åœ–
    n_cols = min(3, len(numeric_cols))
    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
    if n_rows == 1:
        axes = [axes] if n_cols == 1 else axes
    else:
        axes = axes.flatten()
    
    outlier_summary = {}
    
    for i, col in enumerate(numeric_cols):
        if i < len(axes):
            # ç®±å‹åœ–
            axes[i].boxplot(df[col].dropna())
            axes[i].set_title(f'{col} ç®±å‹åœ–')
            axes[i].set_ylabel(col)
            
            # IQR æ–¹æ³•æª¢æ¸¬ç•°å¸¸å€¼
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]
            outlier_summary[col] = {
                'ç•°å¸¸å€¼æ•¸é‡': len(outliers),
                'ç•°å¸¸å€¼æ¯”ä¾‹': len(outliers) / len(df) * 100,
                'ä¸‹ç•Œ': lower_bound,
                'ä¸Šç•Œ': upper_bound
            }
    
    # éš±è—å¤šé¤˜çš„å­åœ–
    for i in range(len(numeric_cols), len(axes)):
        axes[i].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/outlier_boxplots.png", dpi=300, bbox_inches='tight')
    plt.close()
    
    # ç•°å¸¸å€¼æ‘˜è¦
    print("\nç•°å¸¸å€¼æ‘˜è¦ (IQR æ–¹æ³•):")
    outlier_df = pd.DataFrame(outlier_summary).T
    print(outlier_df)
    
    return outlier_df

def generate_summary_report(df, missing_df, outlier_df, output_dir):
    """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""
    print("ğŸ“‹ ç”Ÿæˆæ‘˜è¦å ±å‘Š")
    
    report = f"""
========================================
æ¢ç´¢æ€§è³‡æ–™åˆ†æå ±å‘Š
========================================
åˆ†ææ—¥æœŸ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

è³‡æ–™æ¦‚è¦:
- ç¸½è¨˜éŒ„æ•¸: {df.shape[0]:,}
- ç¸½æ¬„ä½æ•¸: {df.shape[1]}
- æ•¸å€¼æ¬„ä½: {len(df.select_dtypes(include=[np.number]).columns)}
- é¡åˆ¥æ¬„ä½: {len(df.select_dtypes(include=['object', 'category']).columns)}

è³‡æ–™å“è³ª:
- å®Œæ•´è¨˜éŒ„: {df.dropna().shape[0]:,} ({100*df.dropna().shape[0]/df.shape[0]:.1f}%)
- æœ‰ç¼ºå¤±å€¼è¨˜éŒ„: {df.isnull().any(axis=1).sum():,} ({100*df.isnull().any(axis=1).sum()/df.shape[0]:.1f}%)
- é‡è¤‡è¨˜éŒ„: {df.duplicated().sum():,} ({100*df.duplicated().sum()/df.shape[0]:.1f}%)

ç¼ºå¤±å€¼æœ€åš´é‡çš„æ¬„ä½:
"""
    
    if len(missing_df) > 0:
        for idx, row in missing_df.head(5).iterrows():
            report += f"- {idx}: {row['ç¼ºå¤±æ•¸é‡']} ({row['ç¼ºå¤±ç™¾åˆ†æ¯”']:.1f}%)\n"
    else:
        report += "- ç„¡ç¼ºå¤±å€¼\n"
    
    report += "\nç•°å¸¸å€¼æœ€å¤šçš„æ¬„ä½:\n"
    if len(outlier_df) > 0:
        for idx, row in outlier_df.sort_values('ç•°å¸¸å€¼æ•¸é‡', ascending=False).head(5).iterrows():
            report += f"- {idx}: {row['ç•°å¸¸å€¼æ•¸é‡']} ({row['ç•°å¸¸å€¼æ¯”ä¾‹']:.1f}%)\n"
    else:
        report += "- ç„¡æ•¸å€¼æ¬„ä½é€²è¡Œç•°å¸¸å€¼åˆ†æ\n"
    
    report += f"""
å»ºè­°å¾ŒçºŒåˆ†æ:
1. è™•ç†ç¼ºå¤±å€¼ (å¡«è£œæˆ–ç§»é™¤)
2. è™•ç†ç•°å¸¸å€¼ (è½‰æ›æˆ–ç§»é™¤)  
3. ç‰¹å¾µå·¥ç¨‹ (ç·¨ç¢¼ã€æ¨™æº–åŒ–)
4. é€²ä¸€æ­¥çš„çµ±è¨ˆåˆ†ææˆ–å»ºæ¨¡

ç”Ÿæˆçš„è¦–è¦ºåŒ–æª”æ¡ˆ:
- missing_values.png: ç¼ºå¤±å€¼åˆ†æ
- numerical_distributions.png: æ•¸å€¼è®Šæ•¸åˆ†å¸ƒ
- correlation_matrix.png: ç›¸é—œæ€§çŸ©é™£
- categorical_*.png: é¡åˆ¥è®Šæ•¸åˆ†å¸ƒ
- outlier_boxplots.png: ç•°å¸¸å€¼ç®±å‹åœ–
========================================
"""
    
    # å„²å­˜å ±å‘Š
    with open(f"{output_dir}/eda_report.txt", "w", encoding='utf-8') as f:
        f.write(report)
    
    print("âœ… æ‘˜è¦å ±å‘Šå·²å„²å­˜")

def main():
    """ä¸»è¦åŸ·è¡Œæµç¨‹"""
    print("ğŸ“Š æ¢ç´¢æ€§è³‡æ–™åˆ†ææµç¨‹")
    print("=" * 40)
    
    # è¨­å®šåƒæ•¸
    DATA_FILE = "data/dataset.csv"  # ä¿®æ”¹ç‚ºæ‚¨çš„è³‡æ–™æª”æ¡ˆè·¯å¾‘
    OUTPUT_DIR = "results/eda"
    
    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
    if not os.path.exists(DATA_FILE):
        print(f"âŒ è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: {DATA_FILE}")
        print("è«‹æº–å‚™ CSV æ ¼å¼çš„è³‡æ–™æª”æ¡ˆ")
        return
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åŸ·è¡Œåˆ†ææµç¨‹
    df = load_and_inspect_data(DATA_FILE)
    missing_df = data_quality_analysis(df, OUTPUT_DIR)
    numerical_analysis(df, OUTPUT_DIR)
    categorical_analysis(df, OUTPUT_DIR)
    outlier_df = outlier_analysis(df, OUTPUT_DIR)
    generate_summary_report(df, missing_df, outlier_df, OUTPUT_DIR)
    
    print("âœ… æ¢ç´¢æ€§è³‡æ–™åˆ†æå®Œæˆ!")
    print(f"ğŸ“‚ çµæœä½ç½®: {OUTPUT_DIR}")
    print(f"ğŸ“‹ æŸ¥çœ‹å ±å‘Š: {OUTPUT_DIR}/eda_report.txt")

if __name__ == "__main__":
    main()
```

## ğŸš€ å·¥ä½œæµç¨‹è‡ªå‹•åŒ–

### é€šç”¨åˆ†ææµç¨‹ç®¡ç†å™¨
```bash
#!/bin/bash
# workflow_manager.sh - é€šç”¨åˆ†ææµç¨‹ç®¡ç†å™¨

set -e

WORKFLOW_DIR="workflows"
RESULTS_DIR="results"
LOG_DIR="logs"

# é¡è‰²å®šç¾©
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥èªŒå‡½æ•¸
log_info() {
    echo -e "${BLUE}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') $1"
}

# å»ºç«‹ç›®éŒ„çµæ§‹
setup_directories() {
    log_info "å»ºç«‹å·¥ä½œæµç¨‹ç›®éŒ„çµæ§‹"
    
    mkdir -p "$WORKFLOW_DIR"
    mkdir -p "$RESULTS_DIR"
    mkdir -p "$LOG_DIR"
    
    # å»ºç«‹æ¨™æº–è³‡æ–™ç›®éŒ„
    mkdir -p data/{raw,processed,reference}
    mkdir -p scripts/{preprocessing,analysis,postprocessing}
    mkdir -p configs
    
    log_success "ç›®éŒ„çµæ§‹å»ºç«‹å®Œæˆ"
}

# åˆ—å‡ºå¯ç”¨å·¥ä½œæµç¨‹
list_workflows() {
    echo "ğŸ” å¯ç”¨çš„å·¥ä½œæµç¨‹:"
    echo "=================="
    
    echo "1. ngs          - NGS åŸºå› çµ„åˆ†ææµç¨‹"
    echo "2. rnaseq       - RNA-seq è¡¨é”åˆ†ææµç¨‹"  
    echo "3. structure    - è›‹ç™½è³ªçµæ§‹é æ¸¬æµç¨‹"
    echo "4. ml-image     - æ©Ÿå™¨å­¸ç¿’åœ–åƒåˆ†é¡æµç¨‹"
    echo "5. ml-nlp       - è‡ªç„¶èªè¨€è™•ç†æµç¨‹"
    echo "6. eda          - æ¢ç´¢æ€§è³‡æ–™åˆ†ææµç¨‹"
    echo "7. custom       - è‡ªè¨‚å·¥ä½œæµç¨‹"
    echo ""
}

# åŸ·è¡Œ NGS å·¥ä½œæµç¨‹
run_ngs_workflow() {
    local sample_name="$1"
    local fastq_dir="$2"
    local reference="$3"
    
    log_info "å•Ÿå‹• NGS å·¥ä½œæµç¨‹: $sample_name"
    
    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
    if [ ! -d "$fastq_dir" ]; then
        log_error "FASTQ ç›®éŒ„ä¸å­˜åœ¨: $fastq_dir"
        return 1
    fi
    
    if [ ! -f "$reference" ]; then
        log_error "åƒè€ƒåŸºå› çµ„æª”æ¡ˆä¸å­˜åœ¨: $reference"
        return 1
    fi
    
    # åŸ·è¡Œåˆ†æ
    log_info "åŸ·è¡Œå®Œæ•´ NGS åˆ†ææµç¨‹..."
    bash workflows/wgs_analysis.sh "$sample_name" "$fastq_dir" "$reference" 2>&1 | tee "$LOG_DIR/ngs_${sample_name}_$(date +%Y%m%d_%H%M%S).log"
    
    log_success "NGS å·¥ä½œæµç¨‹å®Œæˆ: $sample_name"
}

# åŸ·è¡Œçµæ§‹é æ¸¬å·¥ä½œæµç¨‹
run_structure_workflow() {
    local protein_fasta="$1"
    local protein_name="$2"
    
    log_info "å•Ÿå‹•è›‹ç™½è³ªçµæ§‹é æ¸¬å·¥ä½œæµç¨‹: $protein_name"
    
    if [ ! -f "$protein_fasta" ]; then
        log_error "è›‹ç™½è³ªåºåˆ—æª”æ¡ˆä¸å­˜åœ¨: $protein_fasta"
        return 1
    fi
    
    # åŸ·è¡Œçµæ§‹é æ¸¬
    log_info "åŸ·è¡Œ ColabFold çµæ§‹é æ¸¬..."
    bash workflows/protein_structure_prediction.sh "$protein_fasta" "$protein_name" 2>&1 | tee "$LOG_DIR/structure_${protein_name}_$(date +%Y%m%d_%H%M%S).log"
    
    log_success "çµæ§‹é æ¸¬å·¥ä½œæµç¨‹å®Œæˆ: $protein_name"
}

# åŸ·è¡Œæ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹
run_ml_workflow() {
    local workflow_type="$1"
    local data_dir="$2"
    
    log_info "å•Ÿå‹•æ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹: $workflow_type"
    
    case "$workflow_type" in
        "image")
            log_info "åŸ·è¡Œåœ–åƒåˆ†é¡æµç¨‹..."
            python3 workflows/image_classification_pipeline.py 2>&1 | tee "$LOG_DIR/ml_image_$(date +%Y%m%d_%H%M%S).log"
            ;;
        "nlp")
            log_info "åŸ·è¡Œè‡ªç„¶èªè¨€è™•ç†æµç¨‹..."
            python3 workflows/nlp_sentiment_analysis.py 2>&1 | tee "$LOG_DIR/ml_nlp_$(date +%Y%m%d_%H%M%S).log"
            ;;
        *)
            log_error "æœªçŸ¥çš„æ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹é¡å‹: $workflow_type"
            return 1
            ;;
    esac
    
    log_success "æ©Ÿå™¨å­¸ç¿’å·¥ä½œæµç¨‹å®Œæˆ: $workflow_type"
}

# åŸ·è¡Œ EDA å·¥ä½œæµç¨‹
run_eda_workflow() {
    local data_file="$1"
    
    log_info "å•Ÿå‹•æ¢ç´¢æ€§è³‡æ–™åˆ†æå·¥ä½œæµç¨‹"
    
    if [ ! -f "$data_file" ]; then
        log_error "è³‡æ–™æª”æ¡ˆä¸å­˜åœ¨: $data_file"
        return 1
    fi
    
    # åŸ·è¡Œ EDA
    log_info "åŸ·è¡Œæ¢ç´¢æ€§è³‡æ–™åˆ†æ..."
    python3 workflows/exploratory_data_analysis.py 2>&1 | tee "$LOG_DIR/eda_$(date +%Y%m%d_%H%M%S).log"
    
    log_success "EDA å·¥ä½œæµç¨‹å®Œæˆ"
}

# å·¥ä½œæµç¨‹ç‹€æ…‹æª¢æŸ¥
check_workflow_status() {
    local workflow_id="$1"
    
    log_info "æª¢æŸ¥å·¥ä½œæµç¨‹ç‹€æ…‹: $workflow_id"
    
    # æª¢æŸ¥çµæœç›®éŒ„
    if [ -d "$RESULTS_DIR/$workflow_id" ]; then
        echo "ğŸ“‚ çµæœç›®éŒ„: $RESULTS_DIR/$workflow_id"
        echo "ğŸ“Š æª”æ¡ˆæ•¸é‡: $(find "$RESULTS_DIR/$workflow_id" -type f | wc -l)"
        echo "ğŸ’¾ ç›®éŒ„å¤§å°: $(du -sh "$RESULTS_DIR/$workflow_id" | cut -f1)"
    else
        log_warning "çµæœç›®éŒ„ä¸å­˜åœ¨: $RESULTS_DIR/$workflow_id"
    fi
    
    # æª¢æŸ¥æ—¥èªŒæª”æ¡ˆ
    local latest_log=$(ls -t "$LOG_DIR"/*"$workflow_id"* 2>/dev/null | head -1)
    if [ -n "$latest_log" ]; then
        echo "ğŸ“‹ æœ€æ–°æ—¥èªŒ: $latest_log"
        echo "â° æ—¥èªŒæ™‚é–“: $(stat -c %y "$latest_log")"
    else
        log_warning "æ‰¾ä¸åˆ°ç›¸é—œæ—¥èªŒæª”æ¡ˆ"
    fi
}

# æ¸…ç†å·¥ä½œæµç¨‹çµæœ
cleanup_workflow() {
    local workflow_id="$1"
    local confirm="$2"
    
    if [ "$confirm" != "yes" ]; then
        echo "âš ï¸  é€™å°‡åˆªé™¤å·¥ä½œæµç¨‹ '$workflow_id' çš„æ‰€æœ‰çµæœå’Œæ—¥èªŒ"
        echo "å¦‚è¦ç¢ºèªï¼Œè«‹åŸ·è¡Œ: $0 cleanup $workflow_id yes"
        return 1
    fi
    
    log_warning "æ¸…ç†å·¥ä½œæµç¨‹çµæœ: $workflow_id"
    
    # åˆªé™¤çµæœç›®éŒ„
    if [ -d "$RESULTS_DIR/$workflow_id" ]; then
        rm -rf "$RESULTS_DIR/$workflow_id"
        log_info "å·²åˆªé™¤çµæœç›®éŒ„: $RESULTS_DIR/$workflow_id"
    fi
    
    # åˆªé™¤ç›¸é—œæ—¥èªŒ
    find "$LOG_DIR" -name "*$workflow_id*" -type f -delete
    log_info "å·²åˆªé™¤ç›¸é—œæ—¥èªŒæª”æ¡ˆ"
    
    log_success "æ¸…ç†å®Œæˆ: $workflow_id"
}

# ä¸»è¦å‘½ä»¤è™•ç†
main() {
    case "$1" in
        "setup")
            setup_directories
            ;;
        "list")
            list_workflows
            ;;
        "ngs")
            if [ $# -ne 4 ]; then
                echo "ç”¨æ³•: $0 ngs <sample_name> <fastq_dir> <reference_genome>"
                exit 1
            fi
            run_ngs_workflow "$2" "$3" "$4"
            ;;
        "structure")
            if [ $# -ne 3 ]; then
                echo "ç”¨æ³•: $0 structure <protein.fasta> <protein_name>"
                exit 1
            fi
            run_structure_workflow "$2" "$3"
            ;;
        "ml")
            if [ $# -ne 3 ]; then
                echo "ç”¨æ³•: $0 ml <image|nlp> <data_dir>"
                exit 1
            fi
            run_ml_workflow "$2" "$3"
            ;;
        "eda")
            if [ $# -ne 2 ]; then
                echo "ç”¨æ³•: $0 eda <data_file.csv>"
                exit 1
            fi
            run_eda_workflow "$2"
            ;;
        "status")
            if [ $# -ne 2 ]; then
                echo "ç”¨æ³•: $0 status <workflow_id>"
                exit 1
            fi
            check_workflow_status "$2"
            ;;
        "cleanup")
            if [ $# -lt 2 ]; then
                echo "ç”¨æ³•: $0 cleanup <workflow_id> [yes]"
                exit 1
            fi
            cleanup_workflow "$2" "$3"
            ;;
        *)
            echo "ğŸš€ Lab Container å·¥ä½œæµç¨‹ç®¡ç†å™¨"
            echo "================================"
            echo ""
            echo "ç”¨æ³•: $0 <å‘½ä»¤> [åƒæ•¸]"
            echo ""
            echo "å¯ç”¨å‘½ä»¤:"
            echo "  setup                           - å»ºç«‹å·¥ä½œæµç¨‹ç›®éŒ„çµæ§‹"
            echo "  list                            - åˆ—å‡ºå¯ç”¨å·¥ä½œæµç¨‹"
            echo "  ngs <sample> <fastq_dir> <ref>  - åŸ·è¡Œ NGS åˆ†ææµç¨‹"
            echo "  structure <fasta> <name>        - åŸ·è¡Œè›‹ç™½è³ªçµæ§‹é æ¸¬"
            echo "  ml <image|nlp> <data_dir>       - åŸ·è¡Œæ©Ÿå™¨å­¸ç¿’æµç¨‹"
            echo "  eda <data_file.csv>             - åŸ·è¡Œæ¢ç´¢æ€§è³‡æ–™åˆ†æ"
            echo "  status <workflow_id>            - æª¢æŸ¥å·¥ä½œæµç¨‹ç‹€æ…‹"
            echo "  cleanup <workflow_id> [yes]     - æ¸…ç†å·¥ä½œæµç¨‹çµæœ"
            echo ""
            echo "ç¯„ä¾‹:"
            echo "  $0 setup"
            echo "  $0 ngs sample01 data/fastq data/ref/hg38.fa"
            echo "  $0 structure protein.fasta my_protein"
            echo "  $0 ml image data/images"
            echo "  $0 eda data/dataset.csv"
            ;;
    esac
}

# åŸ·è¡Œä¸»å‡½æ•¸
main "$@"
```

---

**é€™äº›å·¥ä½œæµç¨‹ç¯„ä¾‹æ¶µè“‹äº† Lab Container ç³»çµ±çš„ä¸»è¦æ‡‰ç”¨é ˜åŸŸ**  
*å¾åŸºå› çµ„å­¸åˆ°æ©Ÿå™¨å­¸ç¿’ï¼Œå¾è›‹ç™½è³ªçµæ§‹åˆ°è³‡æ–™ç§‘å­¸ï¼Œæä¾›å®Œæ•´çš„åˆ†ææµç¨‹æ¨¡æ¿ï¼*